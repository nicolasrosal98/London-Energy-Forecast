{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SM_London - Data Cleaning / Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from prince import FAMD\n",
    "import plotly.express as pex\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing Information_Households.csv\n",
    "infohouse_df = pd.read_csv('/Users/nicolasrosal/Desktop/Machine Learning Projects/SM_London/Info, Acorns & Holidays/informations_households.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a daily_df out of daily_blocks\n",
    "daily_df = pd.DataFrame()\n",
    "\n",
    "for num in range(0, 112):\n",
    "    block = pd.read_csv('/Users/nicolasrosal/Desktop/Machine Learning Projects/SM_London/daily_dataset/daily_dataset/block_' + str(num)+ '.csv')\n",
    "    block = block[['LCLid', 'day', 'energy_sum']] \n",
    "    daily_df = pd.concat([daily_df, block])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a halfhour_df out of hhblocks\n",
    "halfhour_df = pd.DataFrame()\n",
    "\n",
    "for num in range(0, 112):\n",
    "    block = pd.read_csv('/Users/nicolasrosal/Desktop/Machine Learning Projects/SM_London/hhblock_dataset/hhblock_dataset/block_' + str(num) + '.csv')\n",
    "    halfhour_df = pd.concat([halfhour_df, block])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing weather dataframe\n",
    "weather_df = pd.read_csv('/Users/nicolasrosal/Desktop/Machine Learning Projects/SM_London/Weather Data/weather_daily_darksky.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing holiday dataframe\n",
    "holiday_df = pd.read_csv('/Users/nicolasrosal/Desktop/Machine Learning Projects/SM_London/Info, Acorns & Holidays/uk_bank_holidays.csv')\n",
    "holiday_df['Bank holidays'] = pd.to_datetime(holiday_df['Bank holidays'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. infohouse_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data Cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing Head\n",
    "print('Checking head:\\n\\n{}'.format(infohouse_df.head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "print('Checking for NANs:\\n\\n{}'.format(infohouse_df.isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for inconsistencies in data\n",
    "print('Checking statistical data:\\n\\n{}'.format(infohouse_df.describe()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for data types\n",
    "print('Checking data types:\\n\\n{}'.format(infohouse_df.dtypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature Engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop file column\n",
    "infohouse_df.drop('file', axis = 1, inplace = True)\n",
    "infohouse_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. daily_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data Cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing Head\n",
    "print('Checking head:\\n\\n{}'.format(daily_df.head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "print('Checking for NANs in daily_df:\\n\\n{}'.format(daily_df.isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for inconsistencies in data\n",
    "print('Checking statistical data:\\n\\n{}'.format(daily_df.describe()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for inconsistencies in data\n",
    "print('Checking data types:\\n\\n{}'.format(daily_df.dtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing data type of 'day' column\n",
    "daily_df['day'] = pd.to_datetime(daily_df['day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. halfhour_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data Cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing Head\n",
    "print('Checking head:\\n\\n{}'.format(halfhour_df.head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for missing values\n",
    "print('Checking for NANs in daily_df:\\n\\n{}'.format(halfhour_df.isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for inconsistencies in data\n",
    "print('Checking statistical data:\\n\\n{}'.format(halfhour_df.describe()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for inconsistencies in data\n",
    "print('Checking data types:\\n\\n{}'.format(halfhour_df.dtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SimpleImputer to replace Nans\n",
    "imp_median = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "halfhour_df['hh_19'] = imp_median.fit_transform(halfhour_df[['hh_19']])\n",
    "halfhour_df['hh_25'] = imp_median.fit_transform(halfhour_df[['hh_25']])\n",
    "halfhour_df['hh_26'] = imp_median.fit_transform(halfhour_df[['hh_26']])\n",
    "halfhour_df['hh_30'] = imp_median.fit_transform(halfhour_df[['hh_30']])\n",
    "halfhour_df['hh_36'] = imp_median.fit_transform(halfhour_df[['hh_36']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing datatype of 'day' column\n",
    "halfhour_df['day'] = pd.to_datetime(halfhour_df['day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-Checking for missing values\n",
    "print('Checking for NANs in daily_df:\\n\\n{}'.format(halfhour_df.isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature Engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plot hh averages through the day to find patterns\n",
    "\n",
    "# Compute daily hh averages\n",
    "hh_mean = [mean for mean in halfhour_df.drop(['LCLid', 'day'], axis = 1).mean()] \n",
    "\n",
    "# Create list with times\n",
    "time = [str(i*dt.timedelta(minutes=30)) for i in range(24*60//30)]\n",
    "\n",
    "# Plot means vs times\n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "plt.plot(time, hh_mean)\n",
    "plt.grid()\n",
    "plt.title('Daily Average per Half Hour', fontweight=\"bold\")\n",
    "plt.xlabel('Time of Day', fontweight=\"bold\")\n",
    "plt.xticks(np.arange(0, 50 ,2), rotation = 90, fontsize = 8)\n",
    "plt.ylabel('Half Hour Means', fontweight=\"bold\")\n",
    "plt.yticks(np.arange(0.1, 0.35, 0.05))\n",
    "\n",
    "xcoords = ['4:00:00', '8:30:00', '14:30:00', '19:30:00']\n",
    "for xc in xcoords:\n",
    "    plt.axvline(x=xc, color = 'r', linestyle = '--')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    We identified four patterns in the half hour average per day graph: <b>1) </b> from <b>4:00:00 to 8:30:00</b> there appears to be an ascending trend, <b>2) </b> from <b>8:30:00 to 14:30:00</b> the average stabilizes, <b>3) </b> from <b>14:30:00 to 19:30:00</b> the ascending trend resumes, and <b>4) </b> from <b>19:30:00 to 4:00:00</b> the trend shifts to a downward trend. Hence, we used this four time intervals to group half hour reading by time of day, namely 'Dawn', 'Morning-Afternoon', 'Evening', and 'Night'.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns of halfhour_df\n",
    "columns = ['LCLid', 'day']\n",
    "for hh in time:\n",
    "    columns.append(hh)\n",
    "halfhour_df.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group half hours into 'Dawn', 'Morning-Afternoon', 'Evening', and 'Night' groups\n",
    "# and compute average of group\n",
    "halfhour_df['Avg.Dawn'] = halfhour_df.iloc[:, 11:20].mean(axis=1)\n",
    "halfhour_df['Avg.Morning_Afternoon'] = halfhour_df.iloc[:, 20:32].mean(axis=1)\n",
    "halfhour_df['Avg.Evening'] = halfhour_df.iloc[:, 32:42].mean(axis=1)\n",
    "halfhour_df['Avg.Night'] = halfhour_df.loc[:, list(halfhour_df.columns[2:11]) + \n",
    "           list(halfhour_df.columns[42:-1])].mean(axis = 1)\n",
    "\n",
    "#Update data frame with created columns\n",
    "halfhour_df = halfhour_df[['LCLid', 'day', 'Avg.Dawn', 'Avg.Morning_Afternoon', \n",
    "                       'Avg.Evening', 'Avg.Night']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create holiday column (binary) in halfhour_df\n",
    "halfhour_df['holiday'] = [1 if date in list(holiday_df['Bank holidays']) else 0 for date in halfhour_df['day']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge halfhour_df and infohouse_df\n",
    "halfhour_df = halfhour_df.merge(infohouse_df, on=('LCLid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace Acorn- with most frequent\n",
    "halfhour_df['Acorn'].value_counts() #Identified Acorn-E as most frequent\n",
    "halfhour_df['Acorn'] = halfhour_df['Acorn'].replace('ACORN-', 'ACORN-E')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying changes\n",
    "halfhour_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying changes 2\n",
    "halfhour_df['Acorn'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Merging halfhour_df with daily_df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge halfhour_df and daily_df\n",
    "halfhour_df = halfhour_df.merge(daily_df, on=(['LCLid', 'day']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. weather_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data Cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing Head\n",
    "print('Checking head:\\n\\n{}'.format(weather_df.head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "print('Checking for NANs in daily_df:\\n\\n{}'.format(weather_df.isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for inconsistencies in data\n",
    "print('Checking statistical data:\\n\\n{}'.format(weather_df.describe()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for inconsistencies in data\n",
    "print('Checking data types:\\n\\n{}'.format(weather_df.dtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SimpleImputer to replace Nans\n",
    "imp_median = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "imp_freq = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "weather_df['uvIndex'] = imp_median.fit_transform(weather_df[['uvIndex']])\n",
    "weather_df['cloudCover'] = imp_median.fit_transform(weather_df[['cloudCover']])\n",
    "weather_df['uvIndexTime'] = imp_freq.fit_transform(weather_df[['uvIndexTime']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature Engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating day column and sorting by day\n",
    "weather_df.insert(0, 'day', pd.to_datetime(weather_df['time']).dt.date)\n",
    "weather_df['day'] = pd.to_datetime(weather_df['day'])\n",
    "weather_df.sort_values(by='day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop 'apparent' and 'time' columns\n",
    "weather_df.drop(weather_df.loc[:,weather_df.columns.str.contains(\"apparent\")], axis=1, inplace = True)\n",
    "weather_df.drop(weather_df.loc[:,weather_df.columns.str.contains(\"time\")], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant time columns and keeping sunset and sunrise times\n",
    "weather_df['sunset'] = pd.to_datetime(weather_df['sunsetTime']).dt.round('H').dt.time\n",
    "weather_df['sunrise'] = pd.to_datetime(weather_df['sunriseTime']).dt.round('H').dt.time\n",
    "weather_df.drop(weather_df.loc[:,weather_df.columns.str.contains(\"Time\")], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'summary' columns\n",
    "weather_df.drop('summary', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clustering for Feature Engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating data set exclusively for clustering\n",
    "weatherclust_df = weather_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     Standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing Scaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale non-binary features\n",
    "numerical_features = ['temperatureMax', 'windBearing', 'dewPoint',\n",
    "       'cloudCover', 'windSpeed', 'pressure', 'visibility',\n",
    "       'humidity', 'uvIndex', 'temperatureLow', 'temperatureMin',\n",
    "       'temperatureHigh', 'moonPhase']\n",
    "weatherclust_df[numerical_features] = scaler.fit_transform(weatherclust_df[numerical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying change\n",
    "weatherclust_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     FAMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instantiate and FAMD object\n",
    "famd = FAMD(n_components = 17, engine='auto', check_input=True, random_state=123)\n",
    "famd = famd.fit(weatherclust_df.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring inertia\n",
    "inertia_cumsum = np.cumsum(famd.explained_inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot inertia to find optimal n_components \n",
    "plt.figure(figsize = (9, 6))\n",
    "plt.plot(range(0, 17), inertia_cumsum, marker = 'x', linestyle = '--')\n",
    "plt.title('Explained Variance by Number of Components')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    We decided to keep the number of components that represented 90%+ of the data. Hence, we chose <b>6</b> as the optimal number of components for our FAMD analysis. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate FAMD with optimal number of components (explain 90%+ of the data)\n",
    "famd = FAMD(n_components=6, engine='auto', check_input=True, random_state=123)\n",
    "famd = famd.fit_transform(weatherclust_df.iloc[:, 1:])\n",
    "famd_scores = np.array(famd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for optimal K\n",
    "distortions = []\n",
    "\n",
    "for k in range(1,10):\n",
    "    kmc = KMeans(n_clusters=k, max_iter = 1000, n_init = 20)\n",
    "    kmc.fit(famd_scores)\n",
    "    distortions.append(kmc.inertia_)\n",
    "    \n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(range(1,10), distortions, marker = 'o', linestyle = '--')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow chart showing the optimal k')\n",
    "plt.axvline(x=4, color = 'g', linestyle = '--')\n",
    "plt.axvline(x=5, color = 'r', linestyle = '--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    K = 5 was identified as the optimal k for the KMeans algorithm. Nevertheless, knowing that there are 4 seasons, we decided to shift to optimal <b>K = 4</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing KMeans model with optimal K\n",
    "kmc = KMeans(n_clusters=4, max_iter = 1000, n_init = 20, random_state = 123)\n",
    "\n",
    "#Creating clusters\n",
    "clusters = kmc.fit_predict(famd_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate weather_df with fmad_scores and clusters\n",
    "clusters_df = pd.concat([weatherclust_df.reset_index(drop = True), pd.DataFrame(famd_scores)], axis = 1)\n",
    "clusters_df.columns.values[-6: ] = [f'Component_{i}' for i in range(1,7)]\n",
    "clusters_df['Cluster'] = kmc.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identifying seasons in clusters\n",
    "cluster0 = clusters_df[clusters_df['Cluster'] == 0]['day'].dt.month.value_counts()\n",
    "print('Value counts per month for cluster0:\\n\\n{}\\n\\n'.format(cluster0))\n",
    "cluster1 = clusters_df[clusters_df['Cluster'] == 1]['day'].dt.month.value_counts()\n",
    "print('Value counts per month for cluster1:\\n\\n{}\\n\\n'.format(cluster1))\n",
    "cluster2 = clusters_df[clusters_df['Cluster'] == 2]['day'].dt.month.value_counts()\n",
    "print('Value counts per month for cluster2:\\n\\n{}\\n\\n'.format(cluster2))\n",
    "cluster3 = clusters_df[clusters_df['Cluster'] == 3]['day'].dt.month.value_counts()\n",
    "print('Value counts per month for cluster3:\\n\\n{}\\n\\n'.format(cluster3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "        It is clear that <b>Cluster0</b> and <b>Cluster2</b> can be represented as Summer and Winter, correspondingly. The remianing clusters required an additional interpretation: <b>Cluster1</b> will represent the time of the year in which Autumn and Spring are starting, while <b>Cluster3</b> when they are ending.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Labels to Clusters\n",
    "clusters_df['Season'] = clusters_df['Cluster'].map({0:'Summer', 1:'StartSpr/EndAut', 2:'Winter', 3:'EndSpr/StartAut'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Clusters\n",
    "my_plot = pex.scatter_3d(clusters_df, x = 'Component_1', y = 'Component_2', z = 'Component_3', \n",
    "                         color='Season', title=\"Season Clusters\")\n",
    "my_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combining findings of clustering to weather_df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Season column in weather_df\n",
    "weather_df['Season'] = clusters_df['Season']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify Change\n",
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Merging with daily_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add daily_df data (daily average) to weather_df\n",
    "average_day = daily_df.groupby(daily_df['day'].dt.date).mean()\n",
    "average_day = average_day.reset_index()\n",
    "average_day['day'] = pd.to_datetime(average_day['day'])\n",
    "\n",
    "#Merge halfhour_df and daily_df\n",
    "weather_df = weather_df.merge(average_day, on=(['day']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deleting duplicate rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.drop_duplicates('day', keep = 'first', ignore_index = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Verify\n",
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the final dataset\n",
    "energy_df = halfhour_df.merge(weather_df, on=(['day', 'energy_sum']))\n",
    "energy_df.drop('energy_sum_y', axis = 1, inplace = True)\n",
    "energy_df.rename(columns = {'energy_sum_x': 'energy_sum'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export energy_df\n",
    "energy_df.to_csv('energy_df.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
